# job for downloding the tools-bin package
# created only when the version is set to allow deployment without this
{{ if $.Values.toolsBin.version }}
apiVersion: batch/v1
kind: Job
metadata:
  name: download-tools-bin-{{ $.Values.toolsBin.version | replace "." "-" }}
spec:
  parallelism: 1    
  completions: 1    
  template:         
    metadata:
      name: download-tools-bin-{{ $.Values.toolsBin.version | replace "." "-" }}
    spec:
      containers:
      - env:
        # pass the version string
        # I guess we could simply replace it in the script too
        - name: TOOLS_BIN_VERSION
          value: {{ $.Values.toolsBin.version }}
        name: download-tools-bin-{{ $.Values.toolsBin.version | replace "." "-" }}
        image: {{$.Values.image.chipsterImageRepo}}base
        imagePullPolicy: {{ $.Values.image.localPullPolicy }}
        command: 
        - bash
        - -c
        - |
          
          set -e
          # exit with error when the curl fails, for example when the version doesn't exist
          set -o pipefail

          echo "download tools-bin version ${TOOLS_BIN_VERSION}"
          # the images symlink tools-bin from here
          cd /mnt/tools

          # we'll need a lot of temp space, generally the total size of n largest packages, where n is the number of parallel jobs
          export temp="/mnt/temp"
          export url="https://a3s.fi/swift/v1/AUTH_chipcld/chipster-tools-bin/${TOOLS_BIN_VERSION}/parts"

          # Kill other tasks after first error: "--halt 2" in this old 2014 version 
          # of parallel, equals "--halt now,fail=1" in newer versions.
          #
          # Downlaod packages to local temp file before extraction. We can't pipe directly
          # from the curl to lz4 and tar, because when the files
          # are small, it would take too much time to create all files in pipe's 64k buffer
          # causing the server to timeout the idle connection. 
            
          function download_file {
              file="$1"
            size=$(curl -sI $url/$file | grep -i Content-Length | cut -d ' ' -f 2 | tr -d '\r')
            retries=10
            fail=0
            for i in $(seq 1 $retries); do
              echo "$url/$file $size, try $i" 
              wget --no-verbose --tries 5 $url/$file -O $temp/$file || fail="$?"
              
              if [ $fail != 0 ]; then
                echo "$url/$file $size, try $i download failed"
                continue
              fi
              
              cat $temp/$file | lz4 -d | tar -x || fail="$?"
              if [ $fail != 0 ]; then
                echo "$url/$file $size, try $i extraction failed"
                continue
              fi
              
              rm $temp/$file
              break
            done
            
            if [ "$fail" != 0 ]; then
              echo "echo downloading $url/$file failed $retries times, give up"
              exit $fail
            else
              echo $url/$file $size, try $i completed
            fi
          }

          export -f download_file

          # delete old download temp files
          rm -f $temp/*
            
          # -j 1 downloads packages one by one. Increase it if your volumes are on a storage cluster that can handle lots of parallelism
          if ! curl --fail -s $url/files.txt | grep lz4$ | parallel --ungroup -j 1 --halt 2 "download_file {}"; then
            echo "Downloading files $url/files.txt failed"
            exit 1
          fi
        
        volumeMounts:        
        - mountPath: /mnt/tools
          name: tools-bin
        - mountPath: /mnt/temp
          name: temp
      volumes:
      - name: tools-bin
        persistentVolumeClaim:
          claimName: tools-bin-{{ $.Values.toolsBin.version }}
      # the installation instructions should shown how to stored emptyDirs
      # on a volume, because we need hundreds of gigabytes for the download temp
      - name: temp
        emptyDir: {}
      restartPolicy: OnFailure
{{ end }}